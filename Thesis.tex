\documentclass[master]{outhesis}

\title{Master's Thesis}
\author{Bryan Hoke}
\degreename{Master of Science in Computer Science}
\school{University of Oklahoma}
\chair{Dr. Dean Hougen}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}

\graphicspath{ {Figures/} }

\newcommand{\mutationrate}{0.1}
\newcommand{\crossoverrate}{0.8}
\newcommand{\populationsize}{40}
\newcommand{\learningrulesize}{35}

\begin{document}
\makefrontmatter

% Introduction
\chapter{Introduction}

Chalmers [citation needed] demonstrated that evolutionary processes can produce systems that learn.
However, in Chalmers's work learning was the only phenotypic strategy that could emerge from evolution.
If other strategies, such as hard-coded behavior, were able to emerge alongside the strategy of learning, it might be found that learning is not always the strategy that emerges from evolution.
Presumably, the evolutionary environment (or objective function) would have the effect of influencing which strategies are more likely to emerge; after all, it is reasonable to think that no one strategy would always be optimal for every possible kind of environment.
This work examines how the introduction of a "nurturing" condition -- where mistakes made during learning are not penalized -- into the environment influences the evolution of learning as opposed to an "instinctual" strategy where behavior is hard-coded.

%% Genetic Algorithms
\section{Genetic Algorithms}

%- Motivation

%- Overview

%- Selection operators

%- Reproductive operators

Genetics algorithms are a class of search algorithms inspired by the process of evolution by natural selection.
Solutions to a problem are encoded in strings referred to as chromosomes.
The procedure begins with a population of chromosomes which are typically randomly generated.
The chromosomes are evaluated by an objective function referred to as the fitness function to determine the fitness value of each one.
A process of selection is used to determine the chromosomes which will reproduce into the next generation based on their fitness values.
The selected chromosomes then undergo a process of mutation in order to explore the problem space.
This process is repeated until some stopping condition is met, typically after a certain number of generations.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.1]{GeneticAlgorithm.pdf}
	\caption{A high-level view of a genetic algorithm.}
\end{figure}

%% Artificial Neurons
\section{Artificial Neurons}

Artificial neurons networks are a method of performing computation inspired by the biological neurons found in nature.
An artificial neuron is comprised of a number of weighted input connections, an activation function which implements a response to input, and a number of output connections.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.1]{ArtificialNeuron.pdf}
	\caption{An artificial neuron.}
	\label{fig:neuron}
\end{figure}

Figure \ref{fig:neuron} depicts an artificial neuron with $I$ inputs, one output, and an activation function $f_{AN}$. The neuron receives a vector of $I+1$ input signals, $\mathbf{a}=(a_1, a_2, \ldots, a_I, a_{I+1})$, where $a_{I+1}$ is known as a \emph{bias unit} and always has a value of $-1$.  The input vector is modulated by a weight vector, $\mathbf{w}=(w_1, w_2, \ldots, w_I, w_{I+1})$, where each weight $w_i$ modulates the input signal $a_i$. The neuron computes the net input as the sum of the weighted input signals, giving
\begin{equation}
net=\sum_{i=1}^{I+1}a_iw_i
\end{equation}
The output signal, $o$, is then computed by applying the activation function to the weighted input, so that $o=f_{AN}(net)$.

A common choice for $f_{AN}$ is the sigmoid function:

\begin{equation}
f_{AN}(net) = \frac{1}{1 + e^{-\lambda net}}
\end{equation}

where the parameter $\lambda$ influences the steepness of the function, but usually $\lambda = 1$.

Artificial neurons can be used to compute linearly separable functions without error. This means that, for such a function, there exists at least one threshold value such that the neuron can separate the space of $I$-dimensional input vectors which produce an above-threshold output from those which produce a below-threshold output by an $I$-dimensional hyperplane. This threshold is determined by the bias weight $w_{I+1}$, meaning that the neuron can be used to separate the input vectors for which $net > 0$ from the input vectors for which $net < 0$.

Learning is a technique where the weights of an artificial neuron are updated to realize functions given by data.
One type of learning is known as supervised learning, where the neuron is provided with a data set (known as the training set) consisting of training patterns of input vectors with associated target outputs.
The weights of the neuron are then adjusted until the error between the actual outputs of the neuron and the target outputs in the patterns is minimized.

An example of a supervised learning rule is the so-called delta rule.
The delta rule requires the definition of an error (or objective) function, $\mathcal{E}$, to measure the neuron's error in approximating training targets.
The sum of squared errors is usually used, given by

\begin{equation}
\mathcal{E} = \sum_{p=1}^{P_T}(t_p-o_p)^2
\end{equation}

where $t_p$ and $o_p$ are the target and actual output for the $p$th pattern, and $P_T$ is the total number of patterns in the training set.

Given a single training pattern, weights are updated using

\begin{equation}
w_i(t) = w_i(t - 1) + \Delta w_i(t)
\end{equation}

with

\begin{equation}
\Delta w_i(t) = \eta(- \frac{\partial \mathcal{E}}{\partial w_i})
\end{equation}

where

\begin{equation}
\frac{\partial \mathcal{E}}{\partial w_i} = -2(t_p - o_p)\frac{\partial f_{AN}}{\partial net_p}a_{i,p}
\end{equation}

and $\eta$ is a constant known as the learning rate, $net_p$ is the net input for pattern $p$, and $a_{i,p}$ is the $i$th input signal in pattern $p$.

The delta rule requires that its activation function $f_{AN}$ is continuous and differentiable. If we use the sigmoid function for $f_{AN}$ then

\begin{equation}
\frac{\partial f_{AN}}{\partial net_p} = o_p(1 - o_p)
\end{equation}

giving

\begin{equation}
\frac{\partial \mathcal{E}}{\partial w_i} = -2(t_p - o_p)o_p(1 - o_p)a_{i,p}
\end{equation}

%% Neuroevolution
\section{Neuroevolution}

% Neural network connection strengths, topology, and learning rules can be encoded by strings, meaning they can be evolved by genetic algorithms.
Any feature of an artificial neuron can be subjected to an evolutionary process after being string-encoded,
such as connection weights, architectures, learning rules, and input features [Yao].
However, this work will focus on the evolution of connection weights and learning rules.

The connection weights of a non-learning neuron can be evolved as the hard-coded behavior of that neuron. 
This is analogous to the evolution of instincts or what we think of as "hard-wired" behavior.

The initial connection weights of a learning neuron can be evolved as the initial behavior which will be modified by the neuron's learning rule. 
This can be beneficial for evolving passably fit initial behavior that is then fine-tuned by learning,
but it can also be the case that the values of the initial weights are not adaptive and weights are evolved that are effectively random 
[Note: Mention something from nature as an example, like imprinting?]

Learning rules can be evolved stand-alone by applying them to neurons initialized with random weights in the fitness function.
In this situation the values of the initial weights do not matter because the idea is to evolve learning rules that will converge on a solution that is independent of the values of the starting weights.

The initial connection weights and the learning rules that modify them can be evolved simultaneously.
In this situation, instincts, learning, or a combination of both can all be evolved depending on the problem and the fitness environment.
This work will examine the outcomes of the simultaneous evolution of connection weights and learning rules under different conditions.

%% The Evolution of Learning
\section{The Evolution of Learning}

If an environment is sufficiently diverse or dynamic, instinctual behavior is not reasonably sufficient for good task performance.

Evolved learning behavior that is scalable and adaptive to unknown or changing situations can be seen by countless species in nature. [Shah?]

Evolution is another form of adaptation in addition to learning.

Chalmers evolved learning rules by encoding the parameters of a template formula as strings.

Chalmers demonstrated that the delta learning rule can be evolved in certain situations for supervised networks.

%% Nurturing
\section{Nurturing}

Nurturing is prevalent in the biological world.

Nurturing is defined as "the contribution of time, energy, or other resources by one individual to the expected physical, mental, social, or other development of another individual with which it has an ongoing relationship" [Woehrer].

Nurturing can be an important contributing factor to the evolution of learning (Woehrer et al., 2012; Eskridge \& Hougen, 2012). 

Eskridge \& Hougen (2012) conducted experiments involving food patch estimation in uncertain environments, and their results demonstrated that nurturing as both social learning and safe exploration can promote the evolution of learning. 

Nurturing can cover the initial costs of a learner and the resulting benefits of learning can be paid forward to the next generation. In this way, nurturing can promote the evolution of learning.

Shah [citation] demonstrated that nurturing can promote the evolution of learning in changing environments.

% Hypothesis
\chapter{Hypothesis}

Three factors are considered with respect to their effects on the evolution of learning: the number of tasks, nurturing, and instincts.

Chalmers demonstrated that the likelihood of evolving generalized learning mechanisms is proportional to the number of tasks .

Nurturing alone will have little impact on the evolution of learning because learning is still the only strategy with or without nurturing. The likelihood of evolving generalized learning mechanisms will remain proportional to the number of tasks.

Instincts will be more likely to evolve than learning for small amounts of tasks, but learning will become more likely to evolve than instincts as the number of tasks increases. This will be because there will be a smaller number of instinct genes than learning genes to "tune" to a correct solution for small number of tasks, but because the number of genes required to represent instincts will increase as the number of tasks increases the number of instinct genes will become much larger than the number of learning genes as the number of tasks increases.

The introduction of the ability to evolve initial network weights alongside learning rules allows for the evolution of "instincts" as an alternate strategy to learning. Learning will be more likely to evolve in the nurturing condition than in the non-nurturing condition because of the costs associated with the learning process in the non-nurturing condition. 

% Operational Definitions
\chapter{Operational Definitions}

Instincts are to be represented by the genetically encoded initial weights of a network.

Learning is to be represented by the genetically encoded learning rule that is applied to a network throughout its lifetime.

For evolved networks where both initial weights and learning rules are genetically encoded, instincts are measured by removing the learning rule and evaluating the fitness of the network in the environment in which it was evolved.
For the same networks, learning ability is measured by replacing the genetically encoded initial weights with random initial weights and evaluating the fitness of the network in the environment in which it was evolved; generalized learning ability is measured by evaluating the fitness of this instinct-removed network in an environment different from the one in which it was evolved.

By default, mistakes made by a network during learning will count against its lifetime fitness; this is the "non-nurturing" condition.
In the "nurturing" condition, then, mistakes made by a network during learning will not count against its lifetime fitness.

% Procedure
\chapter{Procedure}

%% Implementation Details
\section{Implementation Details}

%%% Genetic Coding of Learning Mechanisms
\subsection{Genetic Coding of Learning Mechanisms}

\begin{center}
	$a_j$ the activation of the input unit $j$
\end{center}

\begin{center}
	$o_i$ the activation of the output unit $i$
\end{center}

\begin{center}
	$t_i$ the training signal on output unit $i$
\end{center}

\begin{center}
	$w_{ij}$ the current value of the connection strength from input $j$ to output $i$
\end{center}

The genome encodes a function $F$ such that

\[
	\Delta w_{ij} = F(a_j, o_i, t_i, w_{ij})
\]

$F$ is a linear function of its four parameters and their six pairwise products. Thus $F$ is determined by specifying ten coefficients.

The genome encodes these ten coefficients, as well as an eleventh "scale" parameter.

\[
	\Delta w_{ij} = k_0(k_1w_{ij}+k_2a_j+k_3o_i+k_4t_i+k_5w_{ij}a_j+k_6w_{ij}o_i+k_7w_{ij}t_{i}+k_8a_jo_i+k_9a_jt_i+k_{10}o_it_i)
\]

The portion of the genome which encodes $\Delta w_{ij}$ consists of 35 bits. The first five bits encode the scale parameter $k_0$ such that it can represent the values $0$, $\pm 1/256$, $\pm 1/128$, ..., $\pm 32$, $\pm 64$, via exponential encoding. The first bit encodes the sign of $k_0$ (0: negative, 1: positive), and the next four bits encode the magnitude. If these four bits are interpreted as an integer $j$ between 0 and 15, we have

\[
	|k_0|=
	\begin{cases}
		0 & \text{if $j = 0$}\\
		2^{j-9} & \text{if $j = 1, ..., 15$}
	\end{cases}
\]

The other 30 bits encode the other ten coefficients in groups of three. The first bit of each group expresses the sign, and the other two bits express a magnitude of 0, 1, 2, or 4 via a similar exponential encoding. If we interpret these two bits as an integer $j$ between 0 and 3, then

\[
	|k_i|=
	\begin{cases}
		0 & \text{if $j = 0$}\\
		2^{j-1} & \text{if $j = 1, 2, 3$}
	\end{cases}
\]

%%% Genetic Coding of Initial Weights
\subsection{Genetic Coding of Initial Weights}

Network weights are evolved alongside learning rules. It would not be meaningful for each chromosome to represent a single set of weights to be used on all tasks, so instead each chromosome simultaneously represents a distinct set of weights for every task in the evolutionary run. The weights to be applied to each evolutionary task are encoded in distinct, consistent regions of each chromosome. 

\newcommand{\bitsperweight}{3}
\newcommand{\jlen}{2}
\newcommand{\jmin}{0}
\newcommand{\jmax}{3}
\newcommand{\exponentshift}{4}

Weights are encoded using 3 bits each. The first bit is the sign of the weight. If we interpret these two bits as an integer $j$ between 0 and 3, then

\[
	|k_i|=
	\begin{cases}
		0 & \text{if $j = 0$}\\
		2^{j-4} & \text{if $j = 1, 2, 3$}
	\end{cases}
\]

For each task the number of weights encoded is equal to the number of inputs in that task plus one bias weight.

There are two conditions of evaluation: the "nurturing" case and the "non-nurturing" case. During evaluation, each network is first trained for 10 epochs using its learning rule. Following this, the network is evaluated on the same tasks. In the "nurturing" case the individual's fitness is simply the result of the evaluation, whereas in the "non-nurturing" case the individual's fitness is the average of evaluation during all training epochs and the final evaluation.

%%% Evaluation of Fitness
\subsection{Evaluation of Fitness}

Fitness evaluation for the nurturing case:

(1) Create a network with the appropriate number of input units for the task and a single output unit.

(2) Initialize the connection strengths of the network using the values encoded in the chromosome

(3) For 10 epochs, cycle through the training exemplars for the task, where for each exemplar we:

(3a) Propagate input values through the system, yielding output values; then

(3b) Adjust the weights of the system according to the formula specified by the learning procedure, on the basis of inputs, output, training signal, and current weights.

(4) At the end of this process, fitness on the task is measured by testing the network on all training exemplars, and dividing the total error by the number of exemplars, subtracting from 1, and multiplying by 100. This yields a fitness "percentage" between 0 and 100.

Fitness evaluation for the non-nurturing case:

(1) Create a network with the appropriate number of input units for the task and a single output unit.

(2) Initialize the connection strengths of the network using the values encoded in the chromosome

(3) For 10 epochs, cycle through the training exemplars for the task, where for each exemplar we:

(3a) Test the network on the exemplar and measure the error in the network output; then

(3b) Propagate input values through the system, yielding output values, then adjust the weights of the system according to the formula specified by the learning procedure, on the basis of inputs, output, training signal, and current weights.

(4) At the end of this process, test the network on all training exemplars and divide the total error of this test and all tests from step 3a by the total number of tests that occurred, subtracting from 1, and multiplying by 100. This yields a fitness "percentage" between 0 and 100.

Fitness of the chromosome is obtained by evaluating its performance on each of the (typically 20) tasks, and taking the mean fitness over all tasks. In this way every chromosome is assigned a fitness between 0 and 100\%.

%%% Parameters of the Genetic Algorithm
\subsection{Parameters of the Genetic Algorithm}

% TODO: Describe GA used
% TODO: Describe "segment-wise" crossover used

Population size: 40
Crossover rate: 0.8
Mutation rate: 0.01
Elitism factor: 1
Number of generations: 4000

%%% Post-Evolutionary Evaluation
\subsection{Post-Evolutionary Evaluation}

Before each evolutionary run, 10 tasks are selected from the task pool and designated as "test tasks". After each evolutionary run, the chromosome with the highest fitness score in the history of the populations in that run is identified. From this chromosome, the learning rule bits are isolated and evaluated on the test tasks (using both the nurturing and non-nurturing conditions) and the weight-encoding bits are isolated and evaluated on the evolutionary tasks.

% Results
\chapter{Results}

%% Results of initial evolutionary runs
\section{Results of initial evolutionary runs}

\begin{figure}[H]
	\centering
	\includegraphics{ChalmersEvolution.pdf}
	\caption{The evolution of maximum fitness for populations evaluated using 20 tasks under the nurturing and non-nurturing conditions.}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics{ChalmersLearningTest.pdf}
	\caption{The average generalized learning capabilities for the best individuals evolved under the nurturing and non-nurturing conditions.}
\end{figure}

%% Fitness test
\section{Fitness test}

The best individual is re-evaluated on the evolutionary tasks using both the nurturing and non-nurturing conditions (recall that it is evaluated using only one of these conditions during evolution).

In both cases it is clear that the objective function becomes more difficult to satisfy as the number of evolutionary tasks increases. 

Individuals evolved using the non-nurturing case perform better when re-evaluated using the non-nurturing case than when using the nurturing case, and individuals evolved using the nurturing case perform better when re-evaluated using the nurturing case than when using the non-nurturing case.

\begin{figure}[H]
	\centering
	\includegraphics{NonNurturingFitnessTestPlot.pdf}
	\caption{Non-nurturing fitness test on evolutionary tasks for individuals evolved under the nurturing and non-nurturing conditions.}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics{NurturingFitnessTestPlot.pdf}
	\caption{Nurturing fitness test on evolutionary tasks for individuals evolved under the nurturing and non-nurturing conditions.}
\end{figure}

%% Network test
\section{Network test}

The portion of the chromosome which encodes the network weights is isolated and evaluated on the evolutionary tasks by initializing a network with the encoded weights, with the intention of determining the contribution of the portion of the chromosome which encodes the initial weights to the fitness of the whole chromosome.

Networks evolved using the non-nurturing condition performed better on this evaluation than networks evolved using the nurturing condition, suggesting that the initial weights make a larger contribution to fitness in the non-nurturing case than in the nurturing case.
% TODO: Suggest that learning plays a somewhat less important role in the non-nurturing case?

\begin{figure}[H]
	\centering
	\includegraphics{NetworkTestPlot.pdf}
	\caption{Network test on evolutionary tasks for individuals evolved under the nurturing and non-nurturing conditions.}
\end{figure}

%% Learning Improvement
\section{Learning Improvement}

To determine the average amount of useful learning that evolves for a given number of tasks we can examine the difference between the evolutionary fitness and the network fitness.
This is because the network fitness is essentially the same as an individual's fitness before it undergoes learning processes, whereas the evolutionary fitness is an individual's fitness after it undergoes learning processes.

As shown in Figure 4.4, the improvement in fitness due to learning increases as the number of evolutionary tasks increases. This suggests that learning becomes an increasingly important evolutionary strategy as the environment becomes more varied.

\begin{figure}[H]
	\centering
	\includegraphics{LearningImprovementPlot.pdf}
	\caption{The difference between evolutionary fitness and network fitness for individuals evolved under the nurturing and non-nurturing conditions.}
\end{figure}

%% Learning test
\section{Learning test}

The portion of the chromosome which encodes the learning rule is isolated and evaluated on the evolutionary tasks by initializing a network with random weights and the encoded learning rule, with the intention of determining the contribution of the portion of the chromosome which encodes the learning rule to the fitness of the whole chromosome.

In both cases of evaluation the learning rule evolved using the nurturing condition performed better than the learning rule evolved using the non-nurturing condition, suggesting that the learning rule makes a larger contribution to fitness in the nurturing case than in the non-nurturing case.
% TODO: Suggest that learning plays a somewhat more important role in the nurturing case?

\begin{figure}[H]
	\centering
	\includegraphics{NonNurturingLearningTestPlot.pdf}
	\caption{Non-nurturing learning test on evolutionary tasks for individuals evolved under the nurturing and non-nurturing conditions.}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics{NurturingLearningTestPlot.pdf}
	\caption{Nurturing learning test on evolutionary tasks for individuals evolved under the nurturing and non-nurturing conditions.}
\end{figure}

%% Generalization test
\section{Generalization test}

The portion of the chromosome which encodes the learning rule is isolated and evaluated on the test tasks by initializing a network with random weights and the encoded learning rule, with the intention of determining the capacity for generalized learning that was evolved.

In both cases of evaluation the learning rule evolved using the nurturing condition outperformed the learning rule evolved using the non-nurturing condition, suggesting that better generalized learning is more likely to evolve using the nurturing case.

\begin{figure}[H]
	\centering
	\includegraphics{NonNurturingGeneralizationTestPlot.pdf}
	\caption{Non-nurturing generalization test on test tasks for individuals evolved under the nurturing and non-nurturing conditions.}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics{NurturingGeneralizationTestPlot.pdf}
	\caption{Nurturing generalization test on test tasks for individuals evolved under the nurturing and non-nurturing conditions.}
\end{figure}

% Conclusion
\chapter{Conclusion}

The impact of varying the nurturing condition alone was insignificant; fitnesses measured under the nurturing condition were generally higher than those measured under the non-nurturing condition, but this is to be expected because of the additional fitness penalties imposed by the non-nurturing condition.

When instincts and learning were allowed to evolve alongside each other, it was found that instinct-based solutions were more likely to evolve for small numbers of evolutionary tasks, but that learning became more likely to evolve as the number of evolutionary tasks increased.

By varying the nurturing condition when instincts were allowed to evolve alongside learning, it was found that instinct-based solutions were more likely to evolve under the non-nurturing condition whereas learning was more likely to evolve under the nurturing condition.

% Future Work
\chapter{Future Work}

%% The Baldwin Effect
\section{The Baldwin Effect}

Baldwin (1896) argued that learning accelerates evolution because suboptimal individuals increase their baseline fitness by acquiring more adaptive characteristics during life. 
Lifetime learning often involves a cost because the individual may be at risk at an early stage of its life or it may modify its behavior in ways that are not functional for its survival.
Baldwin suggested that evolution tends to select individuals that are born with some of the useful features that would otherwise be learned.
However, a consequence of this effect is that learned features may gradually become assimilated into the genotype and reduce the role of learning;
this is not necessarily a good effect if it is desirable for the individuals to retain the ability to learn after the completion of the evolutionary process. \cite{Floreano:2008wv} 

Nurturing can reduce the evolutionary costs of learning and thus may be able to inhibit the Baldwin effect.
Inhibiting the Baldwin effect may result in evolved individuals that have greater adaptive capabilities.
\makebackmatter
\end{document}